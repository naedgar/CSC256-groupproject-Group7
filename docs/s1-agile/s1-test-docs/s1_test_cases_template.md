# Sprint 1: Test Cases (Group Project â€“ TaskTracker)

These test cases verify that new features introduced in the **Group Project Sprint 1** function correctly and do not break existing functionality from the individual project.

Each test case includes:
- **Test Case ID**
- **Description**
- **Preconditions**
- **Test Steps**
- **Expected Results**
- **Test Type** (Automated, Manual, or Both)

This document maps each case to a user story listed in `tt_user_stories.md`.

* **US001** â€“ Hybrid pytest Organization  
* **US002** â€“ Centralized Task Validation  
* **US003** â€“ Automated TimeService Testing  
* **US004** â€“ Robot Framework Acceptance Suite  
* **US005** â€“ Team-Selected Feature  

> [!NOTE]
> Clearly labeling **Test Type** prepares teams for real-world QA and CI/CD workflows, showing which cases should remain manual and which are automated for continuous validation.

---

## US001 â€“ Hybrid pytest Organization

### ğŸ§ª TC-US001-001: Verify pytest markers execute by scope
* **Description:** Ensure pytest markers (`unit`, `integration`, `e2e`) correctly filter and run only targeted test scopes.
* **Test Type:** Automated  
* **Preconditions:**
  - `pytest.ini` configured with `unit`, `integration`, and `e2e` markers.
  - Concern-based folder structure exists under `/tests/`.
* **Test Steps:**
  1. Run `pytest -m unit`
  2. Run `pytest -m integration`
  3. Run `pytest -m e2e`
* **Expected Result:**
  - Each command runs only its associated test files.
  - No unrelated tests are executed.
* **Status:** â˜ Pass â˜ Fail

---

## US002 â€“ Centralized Task Validation

### ğŸ§ª TC-US002-001: Reject Empty Task Title
* **Description:** Verify that `TaskService` rejects creation of a task with an empty or whitespace-only title.
* **Test Type:** Automated  
* **Preconditions:** Centralized validation implemented in `TaskService` and `schemas.py`.
* **Test Steps:**
  1. POST `/api/tasks` with body:
     ```json
     { "title": " " }
     ```
  2. Observe response.
* **Expected Result:**
  - HTTP 400 Bad Request  
  - JSON error message: `"title required"`
* **Status:** â˜ Pass â˜ Fail

### ğŸ§ª TC-US002-002: Enforce Title Length Limit
* **Description:** Ensure tasks cannot exceed maximum length (e.g., 100 chars).
* **Test Type:** Automated  
* **Preconditions:** API is running with validation active.
* **Test Steps:**
  1. POST `/api/tasks` with 256-character title.
* **Expected Result:**
  - HTTP 400 Bad Request  
  - JSON error message: `"title too long"`
* **Status:** â˜ Pass â˜ Fail

### ğŸ§ª TC-US002-003: Reject Duplicate Title
* **Description:** Prevent duplicate task titles using shared validation logic.
* **Test Type:** Automated  
* **Preconditions:** A task with title â€œBuy milkâ€ already exists.
* **Test Steps:**
  1. POST `/api/tasks` with:
     ```json
     { "title": "Buy milk" }
     ```
* **Expected Result:**
  - HTTP 400 Bad Request  
  - JSON error message: `"Duplicate task"`
* **Status:** â˜ Pass â˜ Fail

---

## US003 â€“ Automated TimeService Testing

### ğŸ§ª TC-US003-001: Validate TimeService API
* **Description:** Verify `/api/time` endpoint returns correct time format and status code.
* **Test Type:** Automated  
* **Preconditions:** App running with `/api/time` implemented.
* **Test Steps:**
  1. Send `GET /api/time`.
* **Expected Result:**
  - HTTP 200 OK  
  - JSON includes fields such as `datetime`, `timezone`.
* **Status:** â˜ Pass â˜ Fail

### ğŸ§ª TC-US003-002: Verify Time Updates in UI
* **Description:** Ensure displayed time on the web page updates automatically.
* **Test Type:** Automated (UI / Playwright)  
* **Preconditions:** Web UI running locally or in CI.
* **Test Steps:**
  1. Launch browser.  
  2. Observe TimeService element on home page.  
  3. Wait 5 seconds and check that time value changes.  
* **Expected Result:**  
  - Time displayed updates within 5 seconds.  
  - Format matches `HH:MM[:SS]`.  
* **Status:** â˜ Pass â˜ Fail

---

## US004 â€“ Robot Framework Acceptance Suite

### ğŸ§ª TC-US004-001: Add Task via Robot Framework
* **Description:** Simulate user adding a task through the web interface using Robot Framework.
* **Test Type:** Automated (Robot)  
* **Preconditions:** Robot suite configured with working URL.  
* **Test Steps:**
  1. Run Robot suite `robot tests/robot/add_task.robot`.
* **Expected Result:**
  - All keywords execute successfully.  
  - `log.html` and `report.html` show Pass.  
* **Status:** â˜ Pass â˜ Fail

### ğŸ§ª TC-US004-002: Delete Task via Robot Framework
* **Description:** Confirm a task can be deleted and no longer appears in the list.
* **Test Type:** Automated (Robot)  
* **Preconditions:** At least one task exists.  
* **Test Steps:**
  1. Run `robot tests/robot/delete_task.robot`.
* **Expected Result:**
  - Task removed successfully.  
  - Robot results = Pass.  
* **Status:** â˜ Pass â˜ Fail

---

## US005 â€“ Team-Selected Feature (Custom)

### ğŸ§ª TC-US005-001: Validate Custom Feature Behavior
* **Description:** Verify team-selected feature works as defined in `group_projects_choice.md`.
* **Test Type:** Automated / Manual (as applicable)  
* **Preconditions:** Feature implemented and accessible.  
* **Test Steps:**  
  1. Execute test steps defined in related user story.  
* **Expected Result:**  
  - Matches acceptance criteria in `tt_user_stories.md`.  
* **Status:** â˜ Pass â˜ Fail

---

## US006 â€“ Regression Verification

### ğŸ§ª TC-US006-001: Ensure All Existing Tests Still Pass
* **Description:** Run full regression suite from individual project to confirm no previous features broke.
* **Test Type:** Automated  
* **Preconditions:** Legacy tests integrated into current repo.  
* **Test Steps:**  
  1. Run all pytest tests without markers:  
     ```bash
     pytest -v
     ```
* **Expected Result:**  
  - All legacy tests report Passed.  
* **Status:** â˜ Pass â˜ Fail

---
